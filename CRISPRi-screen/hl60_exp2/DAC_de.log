
── Column specification ────────────────────────────────────────────────────────
cols(
  sample = col_character(),
  fastq = col_character(),
  lib.type = col_logical(),
  sample.type = col_character(),
  sample.rep = col_double()
)

[1] "count/T0.cnt"        "count/DAC_Rep1.cnt"  "count/DAC_Rep2.cnt" 
[4] "count/DMSO_Rep1.cnt" "count/DMSO_Rep2.cnt"
[1] "Count matrix was filtered down to 8365 rows (initially 57823)."
Best Normalizing transformation with 161 Observations
 Estimated Normality Statistics (Pearson P / df, lower => more normal):
 - arcsinh(x): 1.7678
 - Center+scale: 1.1496
 - Exp(x): 6.3085
 - Log_b(x+a): 1.245
 - orderNorm (ORQ): 1.276
 - sqrt(x + a): 1.1208
 - Yeo-Johnson: 1.2546
Estimation method: Out-of-sample via CV with 10 folds and 5 repeats
 
Based off these, bestNormalize chose:
Standardized sqrt(x + a) Transformation with 161 nonmissing obs.:
 Relevant statistics:
 - a = 4.443749 
 - mean (before standardization) = 1.863057 
 - sd (before standardization) = 0.4290986 
Warning message:
In sqrt(newdata + object$a) : NaNs produced
Saving 7 x 7 in image
Warning message:
Removed 75 rows containing missing values (geom_point). 
[1] "Empirical FDR = 0.017"
