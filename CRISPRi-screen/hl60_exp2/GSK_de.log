
── Column specification ────────────────────────────────────────────────────────
cols(
  sample = col_character(),
  fastq = col_character(),
  lib.type = col_logical(),
  sample.type = col_character(),
  sample.rep = col_double()
)

[1] "count/T0.cnt"        "count/GSK_Rep1.cnt"  "count/GSK_Rep2.cnt" 
[4] "count/DMSO_Rep1.cnt" "count/DMSO_Rep2.cnt"
[1] "Count matrix was filtered down to 7937 rows (initially 49637)."
Best Normalizing transformation with 159 Observations
 Estimated Normality Statistics (Pearson P / df, lower => more normal):
 - arcsinh(x): 1.4831
 - Center+scale: 1.0422
 - Exp(x): 6.8822
 - Log_b(x+a): 1.1835
 - orderNorm (ORQ): 1.257
 - sqrt(x + a): 1.0299
 - Yeo-Johnson: 1.0607
Estimation method: Out-of-sample via CV with 10 folds and 5 repeats
 
Based off these, bestNormalize chose:
Standardized sqrt(x + a) Transformation with 159 nonmissing obs.:
 Relevant statistics:
 - a = 4.805482 
 - mean (before standardization) = 1.894241 
 - sd (before standardization) = 0.4314934 
Warning message:
In sqrt(newdata + object$a) : NaNs produced
Saving 7 x 7 in image
Warning message:
Removed 19 rows containing missing values (geom_point). 
[1] "Empirical FDR = 0.018"
